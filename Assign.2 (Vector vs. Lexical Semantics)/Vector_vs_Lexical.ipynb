{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 2: Vector vs. Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a golden standard G and a large corpus of text C for English language, calculate the average Information Retrieval (IR) metric m of top-k similar words retrieved by the vector semantics based on method v.\n",
    "\n",
    "- G: Report the evaluation results based on the golden standards SimLex-9991.\n",
    "- C: Report the evaluation results based on 2 large corpus from different genres available in NLTK libraries.\n",
    "- v: Report the evaluation results of methods TF-iDF3, Word2Vec4 using the cosine similarity. These methods are also called baselines.\n",
    "- top-k: Report the evaluation results for top-10, i.e., k=10.\n",
    "- m: Report the evaluation results based on average nDCG5 using pytrec-eval-terrier6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import pytrec_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure 1: We select SimLex-999 as our golden truth.\n",
    "1. For each word w, we order the top-10 similar words to w as golden list for w. Note that we may have list of different sizes for each word w. For instance, for ‘soccer’ we may have 3 most similar words and for ‘apple’ we may have 20 most similar words.\n",
    "2. When the size is smaller than 10, we try to expand it by transitivity rule, i.e., w similar-to a, a similar-to b, then w similar-to b. If we don’t reach to top-10, we leave it as it is.\n",
    "3. When the size is greater than 10, we truncate the list to top-10.\n",
    "4. Let’s call the golden top-10 similar words to w as top-k-G[w]; k=10.\n",
    "5. Note the the top-10 list is ordered descending based on the similarity scores in G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure 2: We pick C as our large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure 3: We pick v method (baseline). \n",
    "1. We train v on C.\n",
    "    1. We report the running parameters of v if any.\n",
    "    2. For Word2Vec, we run for context window size {1, 2, 5, 10}, vector size {10, 50, 100, 300}, and iteration number = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure 4: For each word w in our golden standard G, we find the top-10 most similar words according to cosine similarities of vectors based on method v.\n",
    "1. If w is not in our large corpus, then it is unseen words and an instance of OOV. In this assignment, we simply ignore this word.\n",
    "2. If w is in our large corpus, then there are top-10 most similar words that are ordered based on descending order of cosine similarity scores.\n",
    "3. Let’s call the top-10 most similar words of w based on v as top-k-v[w]; k=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure 5: Now we have to compare top-k-G[w] and top-k-v[w] for all w that exists both in golden standard and our large corpus.\n",
    "1. We ask pytrec_eval to calculate ‘nDCG’ as our metric m. The result is for each for w.\n",
    "2. We calculate the average of ‘nDCG’ on all words.\n",
    "3. We report the results on a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure 6: We have to repeat the procedure 3 to 5 for all methods v (baselines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
